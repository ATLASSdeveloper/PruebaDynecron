# üöÄ Mini Asistente de Q&A - Prueba T√©cnica

¬°Hola! üëã Bienvenido/a al repositorio del Mini Asistente de Q&A, una soluci√≥n completa para cargar, buscar y consultar documentos de texto.

## ‚ú® Caracter√≠sticas

- üì§ **Carga de documentos**: Soporta archivos TXT y PDF (entre 3 y 10 archivos)
- üîç **B√∫squeda sem√°ntica**: Encuentra contenido relevante en tus documentos
- ‚ùì **Pregunta y respuesta**: Haz preguntas en lenguaje natural y obt√©n respuestas con citas
- üé® **Interfaz intuitiva**: Dise√±o limpio y f√°cil de usar
- üê≥ **Contenedores Docker**: F√°cil despliegue y ejecuci√≥n

## üõ†Ô∏è Implementaciones

### Backend
- **FastAPI** - Framework web moderno y r√°pido orientado a desarrollo de apis
- **Rate Limiting** - T√©cnica de seguridad para protecci√≥n masiva de solicitudes
- **Persistencia** - La informaci√≥n del contenido subido queda almacenado
- **PyPDF2** - Procesamiento de archivos PDF

### Frontend
- **React** + **TypeScript** - Interfaz de usuario
- **Css** - Dise√±o

### Infraestructura
- **Docker** + **Docker Compose** - Contenedores y orquestaci√≥n
- **oLlama - Tiny** - modelo LLM de ollama ligero

## üöÄ Instrucciones de ejecuci√≥n

### Prerrequisitos
- Docker y Docker Compose instalados
- Git para clonar el repositorio
- Puertos 3000, 8000 y 11434 libres

### Ejecuci√≥n con Docker Compose

1. Clona el repositorio:
```bash
git clone <url-del-repositorio>
cd PruebaDynecron
```

2. Ejecuta el Sistema Completo:
```bash
docker-compose up
```

3. Esperar a que los contenedores est√©n levantados y funcionando

4. Acceder a la web: 
   front - http://localhost:3000 
   back - http://localhost:8000/docs

## üöÄ Decisiones

### Elecci√≥n de Ollama y un modelo local:

    -Decisi√≥n: Utilizar Ollama con un modelo de lenguaje local (espec√≠ficamente el modelo "Tiny") en un contenedor independiente.

    -Justificaci√≥n: Esta arquitectura garantiza que el modelo est√© siempre disponible, sin depender de la latencia o disponibilidad de servicios externos. Se prioriza la autonom√≠a y el control total sobre el entorno de inferencia.

    -Implicaci√≥n: Se asume la responsabilidad de mantener la infraestructura y el rendimiento del modelo, a cambio de eliminar costos variables por uso de API y limitaciones de tasa (rate limiting).

### Selecci√≥n del modelo espec√≠fico "Tiny":

    -Decisi√≥n: Emplear la variante de modelo m√°s ligera disponible.

    -Justificaci√≥n: La elecci√≥n se bas√≥ en la restricci√≥n principal de hardware: la necesidad de ejecutarse de manera eficiente en entornos con recursos limitados (dispositivos con 8 GB de RAM o menos) dentro de un contenedor.

    -Implicaci√≥n: Esta decisi√≥n conlleva una compensaci√≥n (trade-off) entre accesibilidad/eficiencia y la potencia/capacidad del modelo.

### Implementaci√≥n de B√∫squeda en Documentos:

        -Decisi√≥n: Implementar una funcionalidad de b√∫squeda utilizando un procesamiento b√°sico de texto (como coincidencia de palabras clave o t√©rminos) en lugar de una soluci√≥n avanzada con modelos de embeddings, bases de datos vectoriales , etc .

        -Justificaci√≥n: Esta decisi√≥n se tom√≥ para priorizar la simplicidad del desarrollo, reducir la complejidad arquitect√≥nica y minimizar la sobrecarga computacional. El objetivo era obtener una funcionalidad de b√∫squeda m√≠nima viable (MVP) r√°pidamente.

        -Implicaci√≥n: La b√∫squeda ser√° menos inteligente y precisa. No podr√° encontrar conceptos o sin√≥nimos relacionados sem√°nticamente, sino solo coincidencias literales de texto.

## üöÄ Supuestos

###    Supuesto de Autonom√≠a vs. Calidad:

        -Supuesto: Se asumi√≥ que la ventaja de tener un sistema siempre disponible y sin costos operativos variables (autonom√≠a) tendr√≠a m√°s peso que la posible p√©rdida en la calidad de las respuestas en comparaci√≥n con modelos de API m√°s avanzados (como GPT-4).

        -Estado: Parcialmente validado. Se logr√≥ la autonom√≠a, pero la limitaci√≥n en la calidad es m√°s significativa de lo previsto.

###    Supuesto de Costo Total de Propiedad (TCO):

        -Supuesto: Se consider√≥ que el costo de mantener la infraestructura local (tiempo de desarrollo, administraci√≥n y energ√≠a) ser√≠a menor a largo plazo que el costo acumulado de usar APIs de pago por uso para el volumen de peticiones esperado.

        -Estado: Validado para el escenario actual. Dado el bajo volumen o uso experimental, el costo local es efectivamente menor.

###    Supuesto de Hardware M√≠nimo:

        -Supuesto: Se dio por hecho que el hardware base del objetivo (dispositivos con 8 GB de RAM) ser√≠a suficiente para ejecutar el modelo Tiny y la aplicaci√≥n simult√°neamente con un rendimiento aceptable para el usuario.

        -Estado: Validado. El modelo funciona en el hardware objetivo, aunque con limitaciones.

###    Supuesto de Adecuaci√≥n al Uso:

        -Supuesto: Se crey√≥ que las capacidades del modelo Tiny, aunque limitadas, ser√≠an suficientes para el scope funcional del proyecto (por ejemplo, generar respuestas cortas, res√∫menes b√°sicos o clasificaciones simples).

        -Estado: Invalidado. La experiencia pr√°ctica demostr√≥ que el modelo es "poco eficiente con limitaciones t√©cnicas en cuanto a prompt y respuestas, por lo que no es recomendable para proyectos serios". Este es un aprendizaje clave.

### Supuesto sobre el Patr√≥n de B√∫squeda de los Usuarios:

    -Supuesto: Se parti√≥ de la base de que los usuarios realizar√≠an b√∫squedas utilizando palabras clave exactas y espec√≠ficas presentes en los documentos, en lugar de b√∫squedas conceptuales o por similitud sem√°ntica.

    -Estado: Por validar. La efectividad de este enfoque se confirmar√° o refutar√° con el feedback real de los usuarios finales.

## üöÄ Tiempo de desarrollo

14 horas - Estaba empe√±ado en hacer correr un modelo en el contenedor, as√≠ como determinar el script correcto para la descarga autom√°tica del modelo en dado caso no exista ü§™üê≥üíª

## Funciona?

- El sistema es completamente funcional con ciertas limitaciones en el LLM por el modelo ligero utilizado, dando en ocasiones respuestas incorrectas por la incapacidad de procesar toda la informaci√≥n. Por lo que es recomendable utilizar una api lista sin trabajar con modelos locales

## Sistema

### Agregar Documentos

![1 (alt text)](Fotos%20Q&A/Agregar%20Documentos.JPG)
![2 (alt text)](Fotos%20Q&A/Agregar%20Documentos%20Funcional.JPG)

### Buscar Informaci√≥n

![1 (alt text)](Fotos%20Q&A/Buscar%20Documentos.JPG)
![2 (alt text)](Fotos%20Q&A/Buscar%20Documentos%20Funcional.JPG)

### Preguntar con LLM

![1 (alt text)](Fotos%20Q&A/Pregunta%20IA.JPG)
![1 (alt text)](Fotos%20Q&A/Pregunta%20IA%20Funcional%201.JPG)
![1 (alt text)](Fotos%20Q&A/Pregunta%20IA%20Funcional%202.JPG)